{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDense : \n",
    "\n",
    "    def __init__(self , n_input , n_layer):\n",
    "        self.weights = 0.01 * np.random.rand(n_input , n_layer)\n",
    "        self.biases = np.zeros((1,n_layer))\n",
    "\n",
    "\n",
    "    def forward(self , X) : \n",
    "\n",
    "        self.input = X\n",
    "\n",
    "        self.output = np.dot( X , self.weights ) + self.biases \n",
    "\n",
    "\n",
    "    def backward(self , dvalues): \n",
    "\n",
    "        self.dweights = np.dot( self.input.T, dvalues )\n",
    "        self.dbiases = np.sum(dvalues , axis=0 , keepdims=True)\n",
    "\n",
    "        self.dinput = np.dot(dvalues , self.weights.T)\n",
    "\n",
    "\n",
    "class ReLU : \n",
    "\n",
    "    def forward (self , X) : \n",
    "\n",
    "        self.input = X \n",
    "\n",
    "        self.output = np.maximum(0,X)\n",
    "    \n",
    "    def backward(self , dvalues) : \n",
    "\n",
    "        self.dinput = dvalues.copy()\n",
    "\n",
    "        self.dinput[self.input <= 0] = 0\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Softmax : \n",
    "\n",
    "    def forward(self , X) :\n",
    "\n",
    "        exp_values = np.exp(X - np.max(X , axis=1 , keepdims=True)) \n",
    "\n",
    "        probability = exp_values / np.sum(exp_values , axis=1 , keepdims=1)\n",
    "        self.output = probability\n",
    "\n",
    "\n",
    "class loss :\n",
    "\n",
    "    def calculate(self , X , y_true) : \n",
    "\n",
    "        loss = self.forward( X , y_true)\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "\n",
    "class CategorycalEntropyLoss(loss) : \n",
    "\n",
    "    def forward(self , y_pred , y_true) : \n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred , 1e-7 , 1 - 1e-7 )\n",
    "\n",
    "        if len(y_true.shape) == 1 : \n",
    "            \n",
    "            right_probability = y_pred_clipped[range(len(y_pred)) , y_true]\n",
    "        \n",
    "        elif len(y_true.shape) == 2 : \n",
    "\n",
    "            right_probability = np.sum(y_pred_clipped * y_true , axis=1)\n",
    "\n",
    "    \n",
    "        negative_log = -np.log(right_probability)\n",
    "        \n",
    "        return negative_log\n",
    "    \n",
    "    \n",
    "\n",
    "class entropyloss_and_softmax:\n",
    "\n",
    "    def __init__(self) : \n",
    "        self.activation = Softmax()\n",
    "        self.loss = CategorycalEntropyLoss()\n",
    "\n",
    "    def forward (self , input , y_true) :\n",
    "\n",
    "        self.activation.forward(input) \n",
    "\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output , y_true)\n",
    "    \n",
    "\n",
    "    def backward(self , dvalues , y_true) : \n",
    "\n",
    "        sample = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2 : \n",
    "            y_true = np.argmax(y_true , axis=1)\n",
    "        \n",
    "        self.dinput = dvalues.copy()\n",
    "\n",
    "        self.dinput[range(sample) , y_true] -= 1\n",
    "\n",
    "        self.dinput = self.dinput / sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimazer_SDG : \n",
    "\n",
    "    def __init__(self , learning_rate = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def optimazer(self , layer) : \n",
    "\n",
    "        layer.weights +=  -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimazer_decay : \n",
    "\n",
    "    def __init__(self , decay , learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iteration = 1\n",
    "\n",
    "    def pre_optimize(self) :\n",
    "\n",
    "        if self.decay : \n",
    "\n",
    "            self.current_learning_rate = self.learning_rate / (1 + self.decay * self.iteration)\n",
    "\n",
    "    def optimazer(self , layer) : \n",
    "\n",
    "        layer.weights +=  -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    def post_optimizer(self) : \n",
    "\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimazer_decay_momentum : \n",
    "\n",
    "    def __init__(self , decay , momentum , learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iteration = 1\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_optimize(self) :\n",
    "\n",
    "\n",
    "        if self.decay : \n",
    "\n",
    "            self.current_learning_rate = self.learning_rate / (1 + self.decay * self.iteration)\n",
    "\n",
    "\n",
    "    def optimazer(self, layer):\n",
    "\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, \"weight_momentum\"):\n",
    "\n",
    "                layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentum = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentum - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentum = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentum - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentum = bias_updates\n",
    " \n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights \n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "\n",
    "    def post_optimizer(self) : \n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad : \n",
    "\n",
    "    def __init__(self , decay=0 , learning_rate=1 , epsilon=1e-7):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iteration = 0\n",
    "\n",
    "    def pre_optimize(self) : \n",
    "\n",
    "        if self.decay :\n",
    "\n",
    "            self.current_learning_rate = self.learning_rate / (1 + self.decay * self.iteration) \n",
    "\n",
    "    def optimazer(self , layer) : \n",
    "\n",
    "        if not hasattr(layer , \"weight_cache\") : \n",
    "\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / ( np.sqrt(layer.weight_cache) + self.epsilon )\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "\n",
    "    def post_optimizer(self) : \n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_optimize(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def optimazer(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                             (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                           (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_optimizer(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adam : \n",
    "\n",
    "    def __init__(self, learning_rate =0.001, decay=0 , epsilon=1e-7 , beta_1 = 0.9 , beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    \n",
    "\n",
    "    def pre_optimize(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import  spiral_data\n",
    "\n",
    "x,y = spiral_data(samples=100 , classes=3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0  , loss : 1.0986117386100822 , accuracy : 0.27666666666666667\n",
      "iteration : 1000  , loss : 1.0326355594633727 , accuracy : 0.45\n",
      "iteration : 2000  , loss : 0.9535570844206355 , accuracy : 0.5166666666666667\n",
      "iteration : 3000  , loss : 0.9009000682920081 , accuracy : 0.55\n",
      "iteration : 4000  , loss : 0.8489119919675346 , accuracy : 0.6066666666666667\n",
      "iteration : 5000  , loss : 0.8207684345871544 , accuracy : 0.6333333333333333\n",
      "iteration : 6000  , loss : 0.7834207268769776 , accuracy : 0.6833333333333333\n",
      "iteration : 7000  , loss : 0.7562210486740308 , accuracy : 0.6766666666666666\n",
      "iteration : 8000  , loss : 0.7369792981214695 , accuracy : 0.6766666666666666\n",
      "iteration : 9000  , loss : 0.7203880586142581 , accuracy : 0.7\n",
      "iteration : 10000  , loss : 0.7071444368452298 , accuracy : 0.7033333333333334\n",
      "iteration : 11000  , loss : 0.6965966382839466 , accuracy : 0.71\n",
      "iteration : 12000  , loss : 0.6871502848990354 , accuracy : 0.6933333333333334\n",
      "iteration : 13000  , loss : 0.6781928753916816 , accuracy : 0.6933333333333334\n",
      "iteration : 14000  , loss : 0.6706445327359442 , accuracy : 0.7\n",
      "iteration : 15000  , loss : 0.6645848819921696 , accuracy : 0.71\n",
      "iteration : 16000  , loss : 0.6582150323084606 , accuracy : 0.7033333333333334\n",
      "iteration : 17000  , loss : 0.65280142929689 , accuracy : 0.7133333333333334\n",
      "iteration : 18000  , loss : 0.6486128643930904 , accuracy : 0.7133333333333334\n",
      "iteration : 19000  , loss : 0.6416611209171286 , accuracy : 0.7\n",
      "iteration : 20000  , loss : 0.6378351744842503 , accuracy : 0.71\n",
      "iteration : 21000  , loss : 0.6333783427059443 , accuracy : 0.7033333333333334\n",
      "iteration : 22000  , loss : 0.6296182540438346 , accuracy : 0.7066666666666667\n",
      "iteration : 23000  , loss : 0.6241234260182137 , accuracy : 0.72\n",
      "iteration : 24000  , loss : 0.6186310727497318 , accuracy : 0.71\n",
      "iteration : 25000  , loss : 0.6166740065431986 , accuracy : 0.6866666666666666\n",
      "iteration : 26000  , loss : 0.6119604760819125 , accuracy : 0.7066666666666667\n",
      "iteration : 27000  , loss : 0.6088175814140904 , accuracy : 0.7166666666666667\n",
      "iteration : 28000  , loss : 0.6062637078377313 , accuracy : 0.71\n",
      "iteration : 29000  , loss : 0.6028349715032175 , accuracy : 0.7233333333333334\n"
     ]
    }
   ],
   "source": [
    "layer1 = LayerDense(2,64)\n",
    "relu = ReLU()\n",
    "layer2 = LayerDense(64 , 3)\n",
    "softmax_loss = entropyloss_and_softmax()\n",
    "optim = Optimizer_RMSprop()\n",
    "epocs = 30000\n",
    "\n",
    "\n",
    "for i in range(epocs) : \n",
    "\n",
    "    layer1.forward(x)\n",
    "    relu.forward(layer1.output)\n",
    "    layer2.forward(relu.output)\n",
    "\n",
    "    loss =  softmax_loss.forward(layer2.output , y)\n",
    "\n",
    "    prediction = np.argmax(softmax_loss.output , axis=1)\n",
    "    acc = np.mean(prediction == y)\n",
    "    softmax_loss.backward(softmax_loss.output , y)\n",
    "\n",
    "    layer2.backward(softmax_loss.dinput)\n",
    "\n",
    "    relu.backward(layer2.dinput)\n",
    "    layer1.backward(relu.dinput)\n",
    "\n",
    "    optim.pre_optimize()\n",
    "    optim.optimazer(layer1)\n",
    "    optim.optimazer(layer2)\n",
    "    optim.post_optimizer()\n",
    "  \n",
    "\n",
    "    if i % 1000 == 0 : \n",
    "\n",
    "      print(f\"iteration : {i}  , loss : {loss} , accuracy : {acc}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
